{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "cd6c6180",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "7fe055ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_1=\"v2_secondrun_results/bulk_session1.csv\"\n",
    "path_2= \"v2_secondrun_results/bulk_session2.csv\"\n",
    "df_s1 = pd.read_csv(path_1)\n",
    "df_s2 = pd.read_csv(path_2)\n",
    "uniqueIDs = df_s1[\"ppID\"].unique()\n",
    "df_type_check = input(\"Do you want the data split into session 1 and session 2 dataframes or would you like them stacked?\\nPlease answer with either 'split' or 'stacked'.\")\n",
    "destination = \"v2_secondrun_results/Session_1 Session_2_split\"\n",
    "\n",
    "\n",
    "# def keep_second_block(df, column=\"trial_index\", marker=\"0\"):\n",
    "#     if column not in df.columns:\n",
    "#         return df\n",
    "    \n",
    "#     normalized_col = df[column].astype(str).str.strip().str.lower()\n",
    "\n",
    "#     # Also get numeric equivalents (to catch mixed types)\n",
    "#     numeric_col = pd.to_numeric(df[column], errors=\"coerce\")\n",
    "\n",
    "#     # --- Step 3: create boolean mask that catches both text \"0\" and numeric 0 ---\n",
    "#     matches = (normalized_col == str(marker).strip().lower()) | (numeric_col == pd.to_numeric(marker, errors=\"coerce\"))\n",
    "\n",
    "#     positions = df.index[matches].tolist()\n",
    "#     print(\"Multiple marker positions exist\", positions)\n",
    "\n",
    "#     if len(positions) >= 2:\n",
    "#         start_index = positions[-1]\n",
    "#         new_df = df.loc[start_index:].reset_index(drop=True)\n",
    "#         print(f\"Second block extracted from starting at row {start_index}\")\n",
    "#         return new_df\n",
    "#     else:\n",
    "#         print(f\"Only Available marker at row: {positions} \")\n",
    "#         return df\n",
    "\n",
    "for id in uniqueIDs:\n",
    "    participant_data = pd.DataFrame([])\n",
    "\n",
    "    if id == \"ppID\":\n",
    "        continue\n",
    "    else:\n",
    "        participant_df_session1 = df_s1[df_s1[\"ppID\"] == id]\n",
    "        participant_df_session2 = df_s2[df_s2[\"ppID\"] == id]\n",
    "        # lets do the splitting logic first based off the input\n",
    "\n",
    "        # if not participant_df_session2.empty and not participant_df_session1.empty:\n",
    "        #     participant_df_session1 = keep_second_block(participant_df_session1)\n",
    "        #     participant_df_session2 = keep_second_block(participant_df_session2)\n",
    "\n",
    "\n",
    "        if df_type_check.lower() == \"split\":\n",
    "            if not participant_df_session1.empty:\n",
    "                participant_df_session1.to_csv(f\"{destination}/PP_{id}_session_1.csv\")\n",
    "                \n",
    "            if not participant_df_session2.empty:\n",
    "                participant_df_session2.to_csv(f\"{destination}/PP_{id}_session_2.csv\")\n",
    "                \n",
    "        # if the dfs need to be stacked\n",
    "        else:\n",
    "            if not participant_df_session1.empty and not participant_df_session2.empty:\n",
    "                participant_data = pd.concat([participant_df_session1, participant_df_session2])\n",
    "                participant_data.to_csv( f\"ppdata/PP_{id}_stacked.csv\")\n",
    "            else:\n",
    "                continue\n",
    "            \n",
    "\n",
    "        \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "0bc10fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = \"v2_secondrun_results/Session_1 Session_2_split\"\n",
    "destination = \"v2_secondrun_results/Session_1 Session_2_split\" ##lets keep it as the folder because we want to overwrite the files in there\n",
    "datasets = os.listdir(destination)\n",
    "\n",
    "for data in datasets:\n",
    "    df = pd.read_csv(f\"{destination}/{data}\")\n",
    "\n",
    "    positions = df.index[df[\"trial_type\"] == \"preload\"].tolist()\n",
    "    start_index = positions[-1]\n",
    "    sliced_df = df.iloc[start_index:].reset_index(drop=True)\n",
    "    sliced_df.to_csv(f\"{destination}/{data}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f0e36e90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ Missing participants summary:\n",
      "  Present in Session 2 but missing in Session 1: ['5e7e1679f8a96f4be83e51e4']\n",
      "  Present in Session 1 but missing in Session 2: ['613b3fb035407b4479f3b4ea', '65fbd867fe676ad5bdb1ae96']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Paths\n",
    "path_1 = \"datav2session1.csv\"\n",
    "path_2 = \"datav2session2.csv\"\n",
    "\n",
    "# Load data\n",
    "df_s1 = pd.read_csv(path_1)\n",
    "df_s2 = pd.read_csv(path_2)\n",
    "\n",
    "# Ensure save directory exists\n",
    "save_dir = \"ppdata\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "# Collect all unique participant IDs across both sessions\n",
    "uniqueIDs = sorted(set(df_s1[\"ppID\"].unique()) | set(df_s2[\"ppID\"].unique()))\n",
    "\n",
    "# Track missing participants\n",
    "missing_in_s1 = []\n",
    "missing_in_s2 = []\n",
    "\n",
    "for pid in uniqueIDs:\n",
    "    # Skip invalid labels if any\n",
    "    if pid == \"ppID\" or pd.isna(pid):\n",
    "        continue\n",
    "\n",
    "    # Filter for participant across both sessions\n",
    "    s1_data = df_s1[df_s1[\"ppID\"] == pid]\n",
    "    s2_data = df_s2[df_s2[\"ppID\"] == pid]\n",
    "\n",
    "    # --- Handle missing cases ---\n",
    "    if s1_data.empty and not s2_data.empty:\n",
    "        missing_in_s1.append(pid)\n",
    "        # Save session 2 data anyway\n",
    "        s2_data.to_csv(os.path.join(save_dir, f\"Data_{pid}_session2.csv\"), index=False)\n",
    "        continue\n",
    "\n",
    "    elif s2_data.empty and not s1_data.empty:\n",
    "        missing_in_s2.append(pid)\n",
    "        # Save session 1 data anyway\n",
    "        s1_data.to_csv(os.path.join(save_dir, f\"Data_{pid}_session1.csv\"), index=False)\n",
    "        continue\n",
    "\n",
    "    elif s1_data.empty and s2_data.empty:\n",
    "        # shouldn't happen, but just skip if so\n",
    "        continue\n",
    "\n",
    "    # --- If both sessions exist ---\n",
    "    s1_data.to_csv(os.path.join(save_dir, f\"Data_{pid}_session1.csv\"), index=False)\n",
    "    s2_data.to_csv(os.path.join(save_dir, f\"Data_{pid}_session2.csv\"), index=False)\n",
    "\n",
    "# --- Log missing participants ---\n",
    "if missing_in_s1 or missing_in_s2:\n",
    "    print(\"⚠️ Missing participants summary:\")\n",
    "    if missing_in_s1:\n",
    "        print(f\"  Present in Session 2 but missing in Session 1: {missing_in_s1}\")\n",
    "    if missing_in_s2:\n",
    "        print(f\"  Present in Session 1 but missing in Session 2: {missing_in_s2}\")\n",
    "else:\n",
    "    print(\"✅ All participants found in both sessions.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
